{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepshikha04yadav/Text-to-Image/blob/main/text-to-image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deepshikha04yadav/Text-to-Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_F3KR2mOLCx",
        "outputId": "57e21c3b-b8bc-48e8-c271-7e903f170cfd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Text-to-Image' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab: Install all dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install diffusers==0.21.0 transformers==4.30.2 accelerate==0.20.3 safetensors==0.3.1 xformers==0.0.20 Pillow==9.5.0 numpy==1.24.4 matplotlib==3.7.2 gradio==4.0.0"
      ],
      "metadata": {
        "id": "2aLQwLXeK_rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"A matching Triton is not available\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from typing import Optional, Tuple, List\n",
        "from datetime import datetime\n",
        "from importlib.metadata import version\n",
        "\n",
        "from diffusers import (\n",
        "    StableDiffusionPipeline,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DDIMScheduler,\n",
        "    LMSDiscreteScheduler\n",
        ")\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "rb4h-0CBLiLY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Stable Diffusion Generator class\n",
        "class StableDiffusionGenerator:\n",
        "    def __init__(self, model_id: str = \"runwayml/stable-diffusion-v1-5\", device: str = \"auto\"):\n",
        "        try:\n",
        "            self.device = self._setup_device(device)\n",
        "            self.dtype = torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
        "\n",
        "            print(f\"üöÄ Initializing Stable Diffusion on {self.device}\")\n",
        "            print(f\"üìä Using precision: {self.dtype}\")\n",
        "\n",
        "            torch_version = version(\"torch\")\n",
        "            diffusers_version = version(\"diffusers\")\n",
        "            print(f\"üì¶ PyTorch version: {torch_version}\")\n",
        "            print(f\"üì¶ Diffusers version: {diffusers_version}\")\n",
        "\n",
        "            self.pipe = self._load_pipeline(model_id)\n",
        "            self.current_scheduler = \"euler_a\"\n",
        "            self.schedulers = {\n",
        "                \"euler_a\": (\"Euler Ancestral\", \"Fast, good for creative images\"),\n",
        "                \"euler\": (\"Euler\", \"Deterministic, consistent results\"),\n",
        "                \"ddim\": (\"DDIM\", \"Classic, good quality, slower\"),\n",
        "                \"dpm_solver\": (\"DPM Solver\", \"High quality, efficient\"),\n",
        "                \"lms\": (\"LMS\", \"Linear multistep, stable\")\n",
        "            }\n",
        "            print(\"‚úÖ Stable Diffusion Generator Ready!\")\n",
        "            print(f\"üìù Available Schedulers: {list(self.schedulers.keys())}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Initialization Error: {str(e)}\")\n",
        "            print(\"Please ensure Visual C++ Redistributable 2015-2022 is installed\")\n",
        "            raise\n",
        "\n",
        "    def _setup_device(self, device: str) -> torch.device:\n",
        "        if device == \"auto\":\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "                print(f\"üéØ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "                print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "            else:\n",
        "                device = \"cpu\"\n",
        "                print(\"üíª Using CPU (GPU not available)\")\n",
        "        return torch.device(device)\n",
        "\n",
        "    def _load_pipeline(self, model_id: str) -> StableDiffusionPipeline:\n",
        "        try:\n",
        "            pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=self.dtype,\n",
        "                safety_checker=None,\n",
        "                requires_safety_checker=False,\n",
        "            )\n",
        "            print(\"üîß Applying Memory Optimizations...\")\n",
        "            pipe.enable_attention_slicing()\n",
        "            print(\"  ‚úì Attention Slicing: Enabled\")\n",
        "            pipe.enable_vae_slicing()\n",
        "            print(\"  ‚úì VAE Slicing: Enabled\")\n",
        "            try:\n",
        "                pipe.enable_xformers_memory_efficient_attention()\n",
        "                print(\"  ‚úì XFormers Attention: Enabled\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö† XFormers: Not available ({e})\")\n",
        "            if self.device.type == \"cuda\":\n",
        "                try:\n",
        "                    pipe = pipe.to(self.device)\n",
        "                    print(\"  ‚úì Full GPU Loading: Success\")\n",
        "                except RuntimeError as e:\n",
        "                    print(\"  ‚ö† GPU Memory Limited: Using CPU Offload\")\n",
        "                    pipe.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe.enable_sequential_cpu_offload()\n",
        "                print(\"  ‚úì CPU Sequential Offload: Enabled\")\n",
        "            return pipe\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
        "\n",
        "    def set_scheduler(self, scheduler_name: str) -> bool:\n",
        "        if scheduler_name not in self.schedulers:\n",
        "            print(f\"‚ùå Unknown scheduler: {scheduler_name}\")\n",
        "            return False\n",
        "        if scheduler_name == self.current_scheduler:\n",
        "            return True\n",
        "        scheduler_map = {\n",
        "            \"euler_a\": EulerAncestralDiscreteScheduler,\n",
        "            \"euler\": EulerDiscreteScheduler,\n",
        "            \"ddim\": DDIMScheduler,\n",
        "            \"dpm_solver\": DPMSolverMultistepScheduler,\n",
        "            \"lms\": LMSDiscreteScheduler\n",
        "        }\n",
        "        try:\n",
        "            scheduler_class = scheduler_map[scheduler_name]\n",
        "            self.pipe.scheduler = scheduler_class.from_config(self.pipe.scheduler.config)\n",
        "            self.current_scheduler = scheduler_name\n",
        "            name, desc = self.schedulers[scheduler_name]\n",
        "            print(f\"üîÑ Scheduler Changed: {name} ({desc})\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Scheduler Error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str = \"\",\n",
        "        width: int = 512,\n",
        "        height: int = 512,\n",
        "        num_inference_steps: int = 20,\n",
        "        guidance_scale: float = 7.5,\n",
        "        seed: Optional[int] = None,\n",
        "        scheduler: str = \"euler_a\"\n",
        "    ) -> Tuple[Image.Image, dict]:\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty\")\n",
        "        self.set_scheduler(scheduler)\n",
        "        if seed is None:\n",
        "            seed = torch.randint(0, 2**32, (1,)).item()\n",
        "        generator = torch.Generator(device=self.device)\n",
        "        generator.manual_seed(seed)\n",
        "        width = (width // 8) * 8\n",
        "        height = (height // 8) * 8\n",
        "        print(f\"üé® Generating: '{prompt[:50]}...'\")\n",
        "        print(f\"üìè Size: {width}x{height}, Steps: {num_inference_steps}, CFG: {guidance_scale}\")\n",
        "        print(f\"üé≤ Seed: {seed}, Scheduler: {scheduler}\")\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            with torch.inference_mode():\n",
        "                if self.device.type == \"cuda\" and self.dtype == torch.float16:\n",
        "                    with autocast(self.device.type):\n",
        "                        result = self.pipe(\n",
        "                            prompt=prompt,\n",
        "                            negative_prompt=negative_prompt if negative_prompt else None,\n",
        "                            width=width,\n",
        "                            height=height,\n",
        "                            num_inference_steps=num_inference_steps,\n",
        "                            guidance_scale=guidance_scale,\n",
        "                            generator=generator\n",
        "                        )\n",
        "                else:\n",
        "                    result = self.pipe(\n",
        "                        prompt=prompt,\n",
        "                        negative_prompt=negative_prompt if negative_prompt else None,\n",
        "                        width=width,\n",
        "                        height=height,\n",
        "                        num_inference_steps=num_inference_steps,\n",
        "                        guidance_scale=guidance_scale,\n",
        "                        generator=generator\n",
        "                    )\n",
        "            generation_time = time.time() - start_time\n",
        "            metadata = {\n",
        "                \"prompt\": prompt,\n",
        "                \"negative_prompt\": negative_prompt,\n",
        "                \"width\": width,\n",
        "                \"height\": height,\n",
        "                \"steps\": num_inference_steps,\n",
        "                \"guidance_scale\": guidance_scale,\n",
        "                \"scheduler\": scheduler,\n",
        "                \"seed\": seed,\n",
        "                \"generation_time\": round(generation_time, 2),\n",
        "                \"device\": str(self.device),\n",
        "                \"dtype\": str(self.dtype)\n",
        "            }\n",
        "            print(f\"‚úÖ Generated in {generation_time:.2f}s\")\n",
        "            return result.images[0], metadata\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            self._cleanup_memory()\n",
        "            raise RuntimeError(\n",
        "                \"GPU Out of Memory! Try: reducing image size, fewer steps, \"\n",
        "                \"or use CPU mode. Current settings may be too demanding.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Generation failed: {str(e)}\")\n",
        "        finally:\n",
        "            self._cleanup_memory()\n",
        "\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if self.device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def get_memory_usage(self) -> dict:\n",
        "        memory_info = {}\n",
        "        if self.device.type == \"cuda\":\n",
        "            memory_info = {\n",
        "                \"allocated_gb\": torch.cuda.memory_allocated() / 1024**3,\n",
        "                \"reserved_gb\": torch.cuda.memory_reserved() / 1024**3,\n",
        "                \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1024**3,\n",
        "                \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            }\n",
        "        else:\n",
        "            memory_info = {\"device\": \"cpu\", \"note\": \"CPU memory tracking not available\"}\n",
        "        return memory_info\n",
        "\n",
        "    def save_image(self, image: Image.Image, metadata: dict, output_dir: str = \"outputs\") -> str:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"sd_gen_{timestamp}_s{metadata['seed']}_{metadata['width']}x{metadata['height']}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        image.save(filepath)\n",
        "        metadata_file = filepath.replace('.png', '_metadata.txt')\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            f.write(\"Stable Diffusion Generation Metadata\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\")\n",
        "            for key, value in metadata.items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "        print(f\"üíæ Saved: {filepath}\")\n",
        "        return filepath"
      ],
      "metadata": {
        "id": "Gq7S9PKcLiI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from utils.text_preprocessing import AdvancedTextPreprocessor\n",
        "\n",
        "prompt = \"a cyberpunk city at night with neon lights\"\n",
        "# self.preprocessor = AdvancedTextPreprocessor()\n",
        "preprocessor = AdvancedTextPreprocessor()\n",
        "result = preprocessor.preprocess_for_stable_diffusion(prompt)\n",
        "\n",
        "print(result[\"prompt\"])\n",
        "print(result[\"negative_prompt\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_HYqhO0Nt8S",
        "outputId": "af811446-c5f2-4ddb-cac7-1fb331e2b46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a cyberpunk city at night with neon lights, highly detailed, high quality, sharp focus\n",
            "low quality, blurry, bad anatomy, worst quality, low resolution, bad proportions, jpeg artifacts, ugly, deformed, distorted, disfigured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI class for Stable Diffusion\n",
        "class StableDiffusionUI:\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.gallery_images = []\n",
        "        self.generation_history = []\n",
        "\n",
        "    def initialize_generator(self, model_choice: str, device_choice: str) -> str:\n",
        "        try:\n",
        "            model_map = {\n",
        "                \"Stable Diffusion 1.5 (Recommended)\": \"runwayml/stable-diffusion-v1-5\",\n",
        "                \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
        "                \"Realistic Vision (RealVisXL)\": \"SG161222/RealVisXL_V4.0\"\n",
        "            }\n",
        "            device_map = {\n",
        "                \"Auto (Recommended)\": \"auto\",\n",
        "                \"GPU (CUDA)\": \"cuda\",\n",
        "                \"CPU (Slower)\": \"cpu\"\n",
        "            }\n",
        "            model_id = model_map.get(model_choice, \"runwayml/stable-diffusion-v1-5\")\n",
        "            device = device_map.get(device_choice, \"auto\")\n",
        "            self.generator = StableDiffusionGenerator(model_id=model_id, device=device)\n",
        "            memory_info = self.generator.get_memory_usage()\n",
        "            memory_text = f\"Memory Usage: {memory_info}\" if memory_info else \"Ready!\"\n",
        "            return f\"‚úÖ Model loaded successfully!\\n{memory_text}\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Initialization failed: {str(e)}\"\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        width: int,\n",
        "        height: int,\n",
        "        steps: int,\n",
        "        guidance: float,\n",
        "        scheduler: str,\n",
        "        seed: int,\n",
        "        save_image: bool\n",
        "    ) -> Tuple[Optional[Image.Image], str, str]:\n",
        "        if self.generator is None:\n",
        "            return None, \"‚ùå Please initialize the model first!\", \"\"\n",
        "        if not prompt.strip():\n",
        "            return None, \"‚ùå Please enter a prompt!\", \"\"\n",
        "        try:\n",
        "            seed = None if seed == -1 else int(seed)\n",
        "            image, metadata = self.generator.generate_image(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                width=width,\n",
        "                height=height,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=guidance,\n",
        "                scheduler=scheduler,\n",
        "                seed=seed\n",
        "            )\n",
        "            info_text = self._format_generation_info(metadata)\n",
        "            saved_path = \"\"\n",
        "            if save_image:\n",
        "                saved_path = self.generator.save_image(image, metadata)\n",
        "            self.generation_history.append(metadata)\n",
        "            self.gallery_images.append(image)\n",
        "            if len(self.gallery_images) > 10:\n",
        "                self.gallery_images = self.gallery_images[-10:]\n",
        "                self.generation_history = self.generation_history[-10:]\n",
        "            return image, info_text, saved_path\n",
        "        except Exception as e:\n",
        "            return None, f\"‚ùå Generation failed: {str(e)}\", \"\"\n",
        "\n",
        "    def _format_generation_info(self, metadata: dict) -> str:\n",
        "        return f\"\"\"\n",
        "‚úÖ Generation Complete!\n",
        "\n",
        "üéØ **Parameters Used:**\n",
        "‚Ä¢ Prompt: {metadata['prompt'][:100]}{'...' if len(metadata['prompt']) > 100 else ''}\n",
        "‚Ä¢ Size: {metadata['width']} √ó {metadata['height']} pixels\n",
        "‚Ä¢ Steps: {metadata['steps']} (more steps = higher quality, slower)\n",
        "‚Ä¢ Guidance Scale: {metadata['guidance_scale']} (higher = follows prompt more closely)\n",
        "‚Ä¢ Scheduler: {metadata['scheduler']}\n",
        "‚Ä¢ Seed: {metadata['seed']} (for reproducible results)\n",
        "\n",
        "‚è±Ô∏è **Performance:**\n",
        "‚Ä¢ Generation Time: {metadata['generation_time']}s\n",
        "‚Ä¢ Device: {metadata['device']}\n",
        "‚Ä¢ Precision: {metadata['dtype']}\n",
        "\"\"\"\n",
        "\n",
        "    def get_example_prompts(self) -> list:\n",
        "        return [\n",
        "            [\"a serene mountain landscape at sunrise, photorealistic, highly detailed\", \"blurry, low quality\"],\n",
        "            [\"portrait of a wise old wizard, fantasy art, digital painting\", \"ugly, deformed\"],\n",
        "            [\"cyberpunk cityscape at night, neon lights, futuristic\", \"daytime, bright\"],\n",
        "            [\"cute cartoon cat wearing a hat, kawaii style\", \"realistic, scary\"],\n",
        "            [\"abstract geometric patterns, colorful, modern art\", \"representational, dull colors\"]\n",
        "        ]\n",
        "\n",
        "    def show_scheduler_info(self, scheduler: str) -> str:\n",
        "        scheduler_info = {\n",
        "            \"euler_a\": \"**Euler Ancestral**: Fast and creative, adds slight randomness for variety\",\n",
        "            \"euler\": \"**Euler**: Deterministic and consistent, same seed = same result\",\n",
        "            \"ddim\": \"**DDIM**: Classic scheduler, high quality but slower\",\n",
        "            \"dpm_solver\": \"**DPM Solver**: Efficient high-quality generation\",\n",
        "            \"lms\": \"**LMS**: Linear multistep, very stable results\"\n",
        "        }\n",
        "        return scheduler_info.get(scheduler, \"Scheduler information not available\")\n",
        "\n",
        "    def get_memory_info(self) -> str:\n",
        "        if self.generator is None:\n",
        "            return \"Model not loaded\"\n",
        "        try:\n",
        "            memory_info = self.generator.get_memory_usage()\n",
        "            if 'allocated_gb' in memory_info:\n",
        "                return f\"\"\"\n",
        "GPU Memory Usage:\n",
        "‚Ä¢ Allocated: {memory_info['allocated_gb']:.2f}GB\n",
        "‚Ä¢ Reserved: {memory_info['reserved_gb']:.2f}GB\n",
        "‚Ä¢ Total Available: {memory_info['total_gb']:.2f}GB\n",
        "‚Ä¢ Usage: {(memory_info['allocated_gb']/memory_info['total_gb']*100):.1f}%\n",
        "                \"\"\"\n",
        "            else:\n",
        "                return \"CPU mode - memory tracking not available\"\n",
        "        except:\n",
        "            return \"Memory info unavailable\"\n",
        "\n",
        "    def create_interface(self) -> gr.Blocks:\n",
        "        with gr.Blocks(\n",
        "            title=\"üé® Educational Stable Diffusion Generator\",\n",
        "            theme=gr.themes.Soft()\n",
        "        ) as interface:\n",
        "            gr.Markdown(\"\"\"\n",
        "            # üé® Educational Stable Diffusion Text-to-Image Generator\n",
        "            **Learn Generative AI concepts while creating images!**\n",
        "            \"\"\")\n",
        "            with gr.Tab(\"üöÄ Setup & Generation\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### üîß Model Setup\")\n",
        "                        model_choice = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"Stable Diffusion 1.5 (Recommended)\",\n",
        "                                \"Stable Diffusion 2.1\",\n",
        "                                \"Realistic Vision (RealVisXL)\"\n",
        "                            ],\n",
        "                            value=\"Stable Diffusion 1.5 (Recommended)\",\n",
        "                            label=\"Model Selection\"\n",
        "                        )\n",
        "                        device_choice = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"Auto (Recommended)\",\n",
        "                                \"GPU (CUDA)\",\n",
        "                                \"CPU (Slower)\"\n",
        "                            ],\n",
        "                            value=\"Auto (Recommended)\",\n",
        "                            label=\"Device Selection\"\n",
        "                        )\n",
        "                        init_btn = gr.Button(\"üöÄ Initialize Model\", variant=\"primary\")\n",
        "                        init_status = gr.Textbox(\n",
        "                            label=\"Initialization Status\",\n",
        "                            placeholder=\"Click Initialize Model to start\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### üìä System Info\")\n",
        "                        memory_btn = gr.Button(\"üìä Check Memory Usage\")\n",
        "                        memory_info = gr.Textbox(\n",
        "                            label=\"Memory Information\",\n",
        "                            placeholder=\"Click to check memory usage\",\n",
        "                            lines=6\n",
        "                        )\n",
        "                gr.Markdown(\"### ‚ú® Image Generation\")\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        prompt = gr.Textbox(\n",
        "                            label=\"üéØ Prompt (Describe what you want)\",\n",
        "                            placeholder=\"a beautiful landscape painting, oil on canvas, detailed\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                        negative_prompt = gr.Textbox(\n",
        "                            label=\"üö´ Negative Prompt (What to avoid)\",\n",
        "                            placeholder=\"blurry, low quality, bad anatomy\",\n",
        "                            lines=2\n",
        "                        )\n",
        "                        generate_btn = gr.Button(\"üé® Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "                    with gr.Column():\n",
        "                        with gr.Accordion(\"üîß Advanced Settings\", open=True):\n",
        "                            with gr.Row():\n",
        "                                width = gr.Slider(256, 1024, 512, step=64, label=\"Width\")\n",
        "                                height = gr.Slider(256, 1024, 512, step=64, label=\"Height\")\n",
        "                            with gr.Row():\n",
        "                                steps = gr.Slider(10, 100, 20, step=1, label=\"Inference Steps\")\n",
        "                                guidance = gr.Slider(1.0, 20.0, 7.5, step=0.5, label=\"Guidance Scale\")\n",
        "                            scheduler = gr.Dropdown(\n",
        "                                choices=[\"euler_a\", \"euler\", \"ddim\", \"dpm_solver\", \"lms\"],\n",
        "                                value=\"euler_a\",\n",
        "                                label=\"Scheduler\"\n",
        "                            )\n",
        "                            scheduler_info = gr.Textbox(\n",
        "                                label=\"Scheduler Information\",\n",
        "                                interactive=False,\n",
        "                                lines=2\n",
        "                            )\n",
        "                            with gr.Row():\n",
        "                                seed = gr.Number(-1, label=\"Seed\")\n",
        "                                save_image = gr.Checkbox(True, label=\"üíæ Save Generated Images\")\n",
        "                with gr.Row():\n",
        "                    output_image = gr.Image(label=\"üñºÔ∏è Generated Image\", type=\"pil\")\n",
        "                with gr.Row():\n",
        "                    generation_info = gr.Textbox(\n",
        "                        label=\"üìù Generation Information\",\n",
        "                        lines=10,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                    saved_path = gr.Textbox(\n",
        "                        label=\"üíæ Saved File Path\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "            with gr.Tab(\"üìö Learning Resources\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                ## üß† Understanding Stable Diffusion\n",
        "                ### What is Diffusion?\n",
        "                Diffusion models learn to gradually remove noise from random data.\n",
        "                ### Key Components:\n",
        "                **üéØ CLIP (Text Encoder)**\n",
        "                **üßÆ U-Net (Denoising Network)**\n",
        "                **üé® VAE (Variational Autoencoder)**\n",
        "                **‚öôÔ∏è Schedulers**\n",
        "                ### Parameter Guide:\n",
        "                **Steps (10-100)**: More steps = higher quality but slower generation\n",
        "                **Guidance Scale (1-20)**: Higher values make the AI follow your prompt more strictly\n",
        "                **Seed**: Controls randomness - same seed + settings = same image\n",
        "                **Resolution**: Higher resolution = more detail but needs more GPU memory\n",
        "                \"\"\")\n",
        "            with gr.Tab(\"üñºÔ∏è Examples & Gallery\"):\n",
        "                gr.Markdown(\"### üé® Example Prompts to Try\")\n",
        "                examples = gr.Examples(\n",
        "                    examples=self.get_example_prompts(),\n",
        "                    inputs=[prompt, negative_prompt],\n",
        "                    label=\"Click any example to load it\"\n",
        "                )\n",
        "                gr.Markdown(\"### üñºÔ∏è Recent Generations\")\n",
        "                gallery = gr.Gallery(\n",
        "                    value=[],\n",
        "                    label=\"Your Generated Images\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "            # Event handlers\n",
        "            init_btn.click(\n",
        "                fn=self.initialize_generator,\n",
        "                inputs=[model_choice, device_choice],\n",
        "                outputs=init_status\n",
        "            )\n",
        "            generate_btn.click(\n",
        "                fn=self.generate_image,\n",
        "                inputs=[prompt, negative_prompt, width, height, steps, guidance, scheduler, seed, save_image],\n",
        "                outputs=[output_image, generation_info, saved_path]\n",
        "            ).then(\n",
        "                fn=lambda: self.gallery_images,\n",
        "                outputs=gallery\n",
        "            )\n",
        "            scheduler.change(\n",
        "                fn=self.show_scheduler_info,\n",
        "                inputs=scheduler,\n",
        "                outputs=scheduler_info\n",
        "            )\n",
        "            memory_btn.click(\n",
        "                fn=self.get_memory_info,\n",
        "                outputs=memory_info\n",
        "            )\n",
        "        return interface"
      ],
      "metadata": {
        "id": "Und7QVenLiGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio interface\n",
        "ui = StableDiffusionUI()\n",
        "interface = ui.create_interface()\n",
        "interface.launch(\n",
        "    share=True,  # Set to True for public sharing\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860,\n",
        "    debug=False,\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "id": "MWt136I-LiDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_pCskrFxKtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"thedevastator/rsicd-image-caption-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "EBFY5QfGxKpo",
        "outputId": "eb66b95c-ed75-49e4-edee-b155c07a7cd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'rsicd-image-caption-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/rsicd-image-caption-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"data/\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"thedevastator/rsicd-image-caption-dataset\",\n",
        "  file_path)\n",
        "\n",
        "print(\"First 5 records:\", df.head())\n"
      ],
      "metadata": {
        "id": "naOmtOp4zIap",
        "outputId": "85b3ca84-4abb-440b-fba2-ab453e8d4a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3757359953.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m df = kagglehub.load_dataset(\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mKaggleDatasetAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPANDAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m\"thedevastator/rsicd-image-caption-dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     )\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_datasets\u001b[0m  \u001b[0;31m# noqa: PLC0415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             return kagglehub.pandas_datasets.load_pandas_dataset(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mpandas_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpandas_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mfile_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_read_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Now that everything has been validated, we can start downloading and processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36m_validate_read_function\u001b[0;34m(file_extension, sql_query)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;34mf\"Supported file extensions are: {', '.join(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         )\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextension_error_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUPPORTED_READ_FUNCTIONS_BY_EXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class TextImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, captions_dict, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.captions = captions_dict  # {filename: caption}\n",
        "        self.image_files = list(captions_dict.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        caption = self.captions[img_name]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, caption\n"
      ],
      "metadata": {
        "id": "PTwpt8ctw-ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = TextImageDataset(\n",
        "    image_dir=\"data/images\",\n",
        "    captions_dict=your_caption_dictionary,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "8HTJGWKAw-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.attention_gan import AttentionTextToImageGAN, weights_init\n",
        "from models.attention_trainer import AttentionGANTrainer\n",
        "from utils.text_embedding import TextEmbedder\n",
        "\n",
        "# Create model\n",
        "model = AttentionTextToImageGAN(\n",
        "    latent_dim=100,\n",
        "    text_embedding_dim=768,\n",
        "    num_attention_blocks=2,\n",
        "    attention_heads=8\n",
        ")\n",
        "\n",
        "# Initialize\n",
        "model.generator.apply(weights_init)\n",
        "model.discriminator.apply(weights_init)\n",
        "\n",
        "# Train\n",
        "embedder = TextEmbedder()\n",
        "trainer = AttentionGANTrainer(model, embedder)\n",
        "trainer.train(train_loader, num_epochs=100)"
      ],
      "metadata": {
        "id": "GfsRXlY6LiA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python example_usage.py"
      ],
      "metadata": {
        "id": "iaJoEI0ULh-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cd0f31-8417-4717-cb4b-f17be8374e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ATTENTION-BASED TEXT-TO-IMAGE GAN - COMPREHENSIVE EXAMPLE\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PART 1: ATTENTION MECHANISMS EXPLAINED\n",
            "================================================================================\n",
            "\n",
            "üß† ATTENTION MECHANISMS IN OUR GAN:\n",
            "\n",
            "1. SELF-ATTENTION (SAGAN-style)\n",
            "   Purpose: Allow pixels to attend to all other pixels\n",
            "   Benefit: Captures long-range dependencies\n",
            "   \n",
            "   Example: When generating a dog\n",
            "   - Without: Ears and tail might not match body\n",
            "   - With: All body parts are coherent\n",
            "   \n",
            "   Applied at: 8x8, 32x32 resolutions\n",
            "\n",
            "2. CROSS-ATTENTION (Image ‚Üî Text)\n",
            "   Purpose: Image regions attend to relevant text features\n",
            "   Benefit: Better text-image alignment\n",
            "   \n",
            "   Example: \"red car on left, blue house on right\"\n",
            "   - Without: Colors and positions might be mixed\n",
            "   - With: Left region focuses on \"red car\" text\n",
            "          Right region focuses on \"blue house\" text\n",
            "   \n",
            "   Applied at: 16x16, plus in residual blocks\n",
            "\n",
            "3. CBAM (Channel + Spatial Attention)\n",
            "   Purpose: Focus on important channels and locations\n",
            "   Benefit: Better detail and feature emphasis\n",
            "   \n",
            "   Example: Generating sunset\n",
            "   - Channel: Emphasizes warm color channels\n",
            "   - Spatial: Focuses on sky region\n",
            "   \n",
            "   Applied at: 64x64 (final output)\n",
            "\n",
            "4. MULTI-SCALE STRATEGY\n",
            "   8x8:  Global composition\n",
            "   16x16: Object placement\n",
            "   32x32: Detail refinement  \n",
            "   64x64: Final polish\n",
            "\n",
            "\n",
            "üìä Testing Attention Modules:\n",
            "\n",
            "1. Self-Attention\n",
            "   Input:  torch.Size([4, 256, 16, 16])\n",
            "   Output: torch.Size([4, 256, 16, 16])\n",
            "   Gamma (learnable weight): 0.0000\n",
            "   ‚úì Allows each pixel to see all others\n",
            "\n",
            "2. Spatial Cross-Attention\n",
            "   Image Input: torch.Size([4, 256, 16, 16])\n",
            "   Text Input:  torch.Size([4, 768])\n",
            "   Output:      torch.Size([4, 256, 16, 16])\n",
            "   Gamma (learnable weight): 0.0000\n",
            "   ‚úì Image attends to text for better alignment\n",
            "\n",
            "3. Combined Attention Block\n",
            "   Input:  torch.Size([4, 256, 16, 16])\n",
            "   Output: torch.Size([4, 256, 16, 16])\n",
            "   ‚úì Both self and cross-attention applied\n",
            "\n",
            "4. CBAM (Channel + Spatial Attention)\n",
            "   Input:  torch.Size([4, 256, 16, 16])\n",
            "   Output: torch.Size([4, 256, 16, 16])\n",
            "   ‚úì Focuses on important channels and locations\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PART 2: ATTENTION GAN ARCHITECTURE\n",
            "================================================================================\n",
            "\n",
            "üèóÔ∏è  Building Attention GAN...\n",
            "\n",
            "üìã Configuration:\n",
            "   latent_dim: 100\n",
            "   text_embedding_dim: 768\n",
            "   ngf: 64\n",
            "   ndf: 64\n",
            "   num_channels: 3\n",
            "   image_size: 64\n",
            "   num_attention_blocks: 2\n",
            "   attention_heads: 8\n",
            "\n",
            "‚úÖ Model created successfully!\n",
            "\n",
            "üîç Generator Architecture Breakdown:\n",
            "\n",
            "\n",
            "Stage 1: Initial Projection\n",
            "  Input: Noise (100) + Text (768)\n",
            "  ‚Üí Linear + Reshape ‚Üí [batch, 1024, 4, 4]\n",
            "\n",
            "Stage 2: Upsampling to 8x8\n",
            "  ‚Üí ConvTranspose2d (1024 ‚Üí 512)\n",
            "  ‚Üí üéØ SELF-ATTENTION (captures global structure)\n",
            "\n",
            "Stage 3: Upsampling to 16x16  \n",
            "  ‚Üí ConvTranspose2d (512 ‚Üí 256)\n",
            "  ‚Üí üéØ CROSS-ATTENTION with text (object placement)\n",
            "\n",
            "Stage 4: Upsampling to 32x32\n",
            "  ‚Üí ConvTranspose2d (256 ‚Üí 128)\n",
            "  ‚Üí üéØ RESIDUAL ATTENTION BLOCKS (2x)\n",
            "      ‚Ä¢ Self-Attention (spatial coherence)\n",
            "      ‚Ä¢ Cross-Attention (text refinement)\n",
            "  ‚Üí üéØ SELF-ATTENTION (detail coherence)\n",
            "\n",
            "Stage 5: Upsampling to 64x64\n",
            "  ‚Üí ConvTranspose2d (128 ‚Üí 64)\n",
            "  ‚Üí üéØ CBAM (channel + spatial focus)\n",
            "  ‚Üí Conv2d ‚Üí RGB Image (3, 64, 64)\n",
            "\n",
            "üìä Model Statistics:\n",
            "   Generator:      19,256,588 parameters\n",
            "   Discriminator:  8,155,554 parameters\n",
            "   Total:          27,412,142 parameters\n",
            "   Attention Overhead: ~7.4M params\n",
            "\n",
            "================================================================================\n",
            "PART 3: TEXT EMBEDDING & GENERATION\n",
            "================================================================================\n",
            "\n",
            "üîß Initializing components...\n",
            "üîß Loading text embedding model: openai/clip-vit-base-patch32\n",
            "üìç Using device: cuda\n",
            "Loading weights: 100% 196/196 [00:00<00:00, 908.19it/s, Materializing param=text_model.final_layer_norm.weight] \n",
            "\u001b[1mCLIPTextModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
            "Key                                                            | Status     |  | \n",
            "---------------------------------------------------------------+------------+--+-\n",
            "vision_model.encoder.layers.{0...11}.mlp.fc2.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.out_proj.weight | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.layer_norm1.bias          | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.q_proj.bias     | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.q_proj.weight   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.k_proj.bias     | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.k_proj.weight   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.layer_norm2.weight        | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.mlp.fc1.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.mlp.fc2.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.layer_norm2.bias          | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.embeddings.class_embedding                        | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.out_proj.bias   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "text_model.embeddings.position_ids                             | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.layer_norm1.weight        | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "visual_projection.weight                                       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.v_proj.bias     | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.mlp.fc1.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.post_layernorm.bias                               | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.encoder.layers.{0...11}.self_attn.v_proj.weight   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.post_layernorm.weight                             | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.pre_layrnorm.bias                                 | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.embeddings.position_embedding.weight              | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.embeddings.position_ids                           | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "text_projection.weight                                         | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.pre_layrnorm.weight                               | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "vision_model.embeddings.patch_embedding.weight                 | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "logit_scale                                                    | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "‚úÖ Model loaded successfully!\n",
            "üìä Embedding dimension: 512\n",
            "‚úÖ Embedding cache enabled (size: 1000)\n",
            "‚úÖ Components ready!\n",
            "\n",
            "üìù Processing Test Prompts:\n",
            "\n",
            "1. Original: 'a red sports car'\n",
            "   Enhanced: 'a red sports car, highly detailed, high quality, sharp focus...'\n",
            "   Style: None\n",
            "\n",
            "2. Original: 'a beautiful sunset over mountains'\n",
            "   Enhanced: 'a beautiful sunset over mountains, highly detailed, high quality, shar...'\n",
            "   Style: None\n",
            "\n",
            "3. Original: 'a cute cat with blue eyes'\n",
            "   Enhanced: 'a cute cat with blue eyes, highly detailed, high quality, sharp focus...'\n",
            "   Style: None\n",
            "\n",
            "4. Original: 'a modern glass building'\n",
            "   Enhanced: 'a modern glass building, highly detailed, high quality, sharp focus...'\n",
            "   Style: None\n",
            "\n",
            "5. Original: 'a person wearing a blue jacket'\n",
            "   Enhanced: 'a person wearing a blue jacket, highly detailed, high quality, sharp f...'\n",
            "   Style: None\n",
            "\n",
            "üîÑ Creating Text Embeddings...\n",
            "‚úì Embeddings shape: (5, 512)\n",
            "‚úì Embedding dimension: 512\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PART 4: IMAGE GENERATION DEMO\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è  NOTE: Model is randomly initialized (not trained)\n",
            "    Outputs will be noise patterns, not real images.\n",
            "    After training on a dataset, outputs will be high-quality images!\n",
            "\n",
            "üìö Training Requirements:\n",
            "   - Dataset: 10,000+ image-text pairs\n",
            "   - Time: ~30-40 hours on single GPU\n",
            "   - Expected quality: FID ~40-70, CLIP Score ~0.30\n",
            "\n",
            "\n",
            "üé® Generating Sample Images...\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/example_usage.py\", line 284, in <module>\n",
            "    generated = model.generate(text_tensors, num_samples=1)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/models/attention_gan.py\", line 397, in generate\n",
            "    generated_images = self.generator(noise, text_embedding)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/models/attention_gan.py\", line 179, in forward\n",
            "    text_condition = self.text_projection(text_embedding)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (5x512 and 768x256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_HaNAzZLh6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRvmnOd9dyyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}